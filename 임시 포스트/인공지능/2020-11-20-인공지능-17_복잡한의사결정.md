---
title: "인공지능-17. 복잡한 의사결정"
category:
  - AI
tag:
  - AI
toc: true
toc_sticky: true
---

이번 장에서는 내일 다시 결정할 수 잇다는 조건하에서 오늘 무엇을 할지 결정하는 방법들을 살펴본다.

이번 장은 확률론적 환경에서의 의사결정에 관여하는 계산 문제들을 다룬다.

16장에서는 단발적인, 즉 일화적인 의사결정 문제를 고찰했다. 그런 문제에서는 각 동작의 결과의 효용이 잘 알려져 있다. 이번 장에서는 에이전트의 효용이 결정들의 순차열에 의존하는 **순차적 의사결정 문제(sequential decision problem)**를 고찰한다. 순차적 의사결정 문제에는 효용, 불확실성, 감지가 관여한다. 검색 문제와 계획 수립 문제는 순차적 의사결정 문제의 특수 경우들이다.




# 순차적 의사결정 문제



# 평가치 반복



# 방침 반복



# 부분 관찰 가능 MDP



# 다중 에이전트 의사결정: 게임이론



# 메커니즘 설계











# 요약

이번 장에서는 한 동작의 결과가 불확실하고 행동의 보상을 여러 동작이 지나간 후에야 받을 수도 있는 상황에서도 세계에 대한 지식을 이용해서 결정을 내리는 방법을 살펴보았다. 요점은 다음과 같다.

**마르코프 의사결정 문제(MDP)**라고도 하는 불확실한 환경에서의 순차적 의사결정 문제는 동작들의 확률적 결과들을 지정하는 **전이 모형**과 각 상태의 보상을 지정하는 **보상 함수**로 정의된다.

한 상태열의 효용은 그 상태열의 모든 상태의 보상들의 합이다. 그 보상들은 시간에 따라 할인될 수 있다. MDP의 해답은 에이전트가 도달할 수 있는 모든 상태에 각각의 결정을 연관시키는 **방침**이다. 실행 시 발생하는 상태열의 효용이 최대화되는 방침을 최적 방침이라고 부른다.

한 상태의 효용은 그 상태에서 출발해서 최적 방침을 실행했을 때 만나게 될 상태열들의 기대 효용이다. **평가치 반복** 알고리즘은 각 상태의 효용을 그 이웃들의 효용들에 연관시키는 방정식을 반복해서 풀어서 MDP의 해답을 구한다.

**방침 반복**은 현재 방침하에서의 상태들의 효용들을 계산하는 단계와 현재 효용들로 현재 방침을 개선하는 단계를 번갈아 수행한다.

POMDP로 줄여 표기하는 부분 관찰 가능 MDP는 보통의 MDP보다 풀기가 훨씬 어렵다. POMDP는 믿음 상태들의 연속 공간 안의 MDP로 변환해서 풀 수 있다. 그런 형태의 MDP에 대한 평가치 반복 알고리즘과 방침 반복 알고리즘이 고안되어 있다. POMDP의 최적 행동에는 불확실성을 줄이기 위한, 그리고 그럼으로써 나중에 좀 더 나은 결정을 내리기 위한 정보 수집 동작이 포함된다.

POMDP 환경에 대한 결정이론적 에이전트를 구축할 수 있다. 그러한 에이전트는 **동적 의사결정망**을 이용해서 전이 모형과 감지기 모형을 표현하고, 믿음 상태를 갱신하고, 가능한 동작열들을 전방으로 투영한다.

**게임이론**은 여러 에이전트가 동시에 상호작용하는 환경에서의 에이전트의 합리적 행동을 서술한다. 게임의 해답은 그 어떤 플레이어도 지정된 전략에서 벗어나는 것이 이득이 되지 않는 전략 프로파일이다. 그 프로파일은 내시 평형에 해당한다.

**메커니즘 설계**는 합리적 에이전트들의 개별적인 동작을 통해서 어떤 전체 효용을 최대화하기 위해 에이전트들의 상호작용 규칙들을 설정한는 데 사용할 수 있다. 종종, 각 에이턴트가 다른 에이전트의 선택을 고려해야 한다는 요구조건 없이도 그런 목표를 달성할 수 있다.









