---
title: "인공지능-20. 확률 모형의 학습"
category:
  - AI
tag:
  - AI
toc: true
toc_sticky: true
---

실세계의 환경들에는 곳곳에 불확실성이 있다. 에이전트가 확률과 결정이론의 방법들로 불확실성을 처리할 수 있지만, 그러려면 먼저 세계의 확률적 이론들을 경험으로부터 배워야 한다. 이번 장에서는 학습 과제 자체를 확률적 추론 과정으로 형식화함으로써 확률적 이론들을 배우는 방법을 설명한다. 

이번 장을 통해서 학습에 대한 베이즈식 관점이 대단히 강력하며, 잡음, 과대적합, 최적 예측 문제들에 대한 일반적인 해답을 제공한다는 점을 알게 될 것이다.

또한, 이번 장에서는 전지적이지 않은 에이전트는 세계에 대한 이론 중 어떤 것이 옳은지 결코 확실하게 알 수 없지만, 그래도 세계에 대한 어떤 이론을 이용해서 결정을 내려야 한다는 사실에 관해서도 논의한다.

# 통계적 학습



# 완전 자료를 이용한 학습



# 숨겨진 변수들이 있는 학습: EM 알고리즘









# 요약

통계적 학습 방법들은 단순한 평균 계산에서부터 베이즈망 같은 복잡한 모형의 구축에 이르기까지 다양하다. 통계적 학습은 컴퓨터 과학, 공학, 계산 생물학, 신경과학, 심리학, 물리학에 널리 쓰인다. 이번 장에서는 기본적인 착안 몇 가지를 제시하고 그 밑에 깔린 수학을 간략하게만 소개했다.

베이즈식 학습 방법 : 관찰들을 이요해서 가설들에 대한 사전 분포를 갱신함으로써 학습이 일종의 확률적 추리 문제로 형식화한다. 이러한 접근방식은 오컴의 면도날 원리를 실현하는데 적합하나, 복잡한 가설 공간에 대해서는 금세 처리 불가능한 수준이 된다.

최대 사후 확률(MAP) 학습 : 주어진 자료에 대해 가장 그럴듯한 가설 하나를 선택한다. 가설 사전 확률이 여전히 쓰이며, 이 방법이 완전한 베이즈식 학습보다 처리 가능성이 좀 더 좋은 경우가 많다.

최대 가능도 학습 : 그냥 자료의 가능도를 최대화하는 가설들을 선택한다. 사전 확률들이 균등 분포일 때에는 MAP 학습과 동치이다. 선형 회귀나 완전 관찰 가능 베이즈망 같은 간단한 경우에는 닫힌 형식의 최대 가능도 해답을 쉽게 찾을 수 있다. 

단순 베이즈 학습 : 규모가변성이 좋은, 특히나 효과적인 기법이다.

일부 변수가 숨겨져 있을 때에는 최대 가능도 해답을 EM 알고리즘을 이용해서 구할 수 있다. EM의 응용 대상으로는 가우스 호합을 이용한 군집화, 베이즈망, 학습, 은닉 마르코프 모형 학습이 있다.

베이즈망의 구조를 배우는 것은 모형 선택의 한 예이다. 여기에는 흔히 구조들의 공간의 이산적 검색이 관여한다. 일부 방법들에는 모형 복잡도와 적합도 사이의 절충점을 선택할 필요가 있다.

비매개변수적 모형들은 자료점들의 집합을 이용해서 분포를 표현한다. 그래서 매개 변수 개수가 훈련 집합 크기에 따라 증가한다. 최근접 이웃 방법은 질의점에 가장 가까운 견본들을 고려하는 반면 핵 방법들은 모든 견본을 거리 기반 가중치로 결합한다.










