---
title: "인공지능-18. 견본을 통한 학습"
category:
  - AI
tag:
  - AI
toc: true
toc_sticky: true
---

학습(learning)하는 에이전트.

이번 장의 학습 목표 : 주어진 입력-출력 쌍들로부터 새로운 입력에 대한 출력을 예측하는 함수를 배우는 방법. (이러한 학습은 다소 제한적으로 보이지만, 실제로는 응용 범위가 아주 넓다.)

학습이 필요한 이유 : <br>

1. 에이전트가 처할 만한 상황들을 설계자가 모두 예측할 수 없다. ex) ???
1. 시간에 따른 모든 변화를 설계자가 예측할 수 없다. ex) 주식시장 가격변동
1. 프로그래머(사람)가 해법 자체를 프로그램으로 구현하는 방법을 파악할 수는 경우가 있다. ex) 얼굴 인식





# 학습의 여러 형태



# 지도 학습



# 의사결정 트리의 학습



# 최고의 가설의 평가와 선택



# 학습 이론



# 선형 모형을 이용한 회귀와 분류



# 인공 신경망



# 비매개변수적 모형



# 지지 벡터 기계



# 앙상블 학습



# 실용적인 기계 학습













# 요약

견본들로부터 함수를 배우는 귀납적인 학습에 초점을 두었다.

학습의 형태는 에이전트의 성격, 개선할 구성요소, 사용 가능한 피드백에 따라 다양하다.

사용 가능한 피드백이 입력 견본의 정확한 답을 알려 주는 경우의 학습 문제는 지도 학습에 해당한다. 이때 과제는 y = h(x)를 배우는 것이다.
<br>
- 분류 : 이산 값 함수의 학습
- 회귀 : 연속 함수의 학습

귀납 학습에는 견본들에 잘 부합하는 가설을 찾는 과정이 관여한다. 오컴의 면도날에 따르면, 일관된 가설 중 가장 간단한 것을 고르는 것이 바람직하다. 이 과제의 어려움은 가설의 표현 방식에 따라 다르다.

의사결정 트리는 모든 부울 함수를 표현할 수 있다. 정보 이득 발견법은 간단하고 일관된 의사결정 트리를 찾는 효율적인 방법을 제공한다.

학습 알고리즘의 성능은 학습 곡선으로 측정한다. 학습 곡선은 검증 집합에 대한 예측 정확도를 훈련 집합 크기의 함수로 표시한다.

선택할 수 있는 모형이 여러 개일 때에는, 가장 잘 일반화되는 모형을 교차 검증을 이용해서 찾아내면 된다.

모든 오류가 동일하지는 않은 경우가 종종 있다. 손실 함수는 각 오류가 얼마나 나쁜지 알려준다. 그러면 목표는 검증 집합에 대한 손실이 가장 작은 가설을 찾는 것이다.

계산 학습 이론 : 귀납 학습의 표본 복잡도와 계산 복잡도를 분석한다. 가설 언어의 표현력과 학습 용이성 사이에는 절충이 존재한다.

선형 회귀 : 널리 쓰이는 모형이다. 선형 회귀 모형의 최적 매개변수들은 경사 하강 검색으로 찾을 수 있고 구체적으로 계산할 수도 있다.

강한 문턱값을 가진 선형 분류(퍼셉트론)를 간단한 가중치 갱신 규칙으로 훈련시킴으로써 선형 분리 가능인 자료에 적합시킬 수 있다. 선형 분리 가능이 아닌 자료에 대해서는 규칙이 수렴하지 않는다.

로지스틱 회귀 : 퍼셉트론의 강 문턱값을 로지스틱 함수로 정의되는 약 문턱값으로 대체한 것. 선형 분리 가능이 아닌, 잡음 섞인 자료에 대해서도 경사 하강이 잘 작동한다.

신경망 : 복잡한 비선형 함수를 선형 문턱값 단위들의 네트워크로 표현한다.
다층 순방향 신경망 : 단위들이 충분하다면 그 어떤 함수도 표현할 수 있다. 
역전파 알고리즘 : 출력 오차를 최소화하기 위해 매개변수 공간에서의 경사 하강을 구현한다.






