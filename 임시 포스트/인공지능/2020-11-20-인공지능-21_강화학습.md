---
title: "인공지능-21. 강화 학습"
category:
  - AI
tag:
  - AI
toc: true
toc_sticky: true
---


# 소개

에이전트가 성공과 실패로부터, 그리고 보상과 징벌록부터 배우는 방법을 설명한다.

18~20장에서는 견본들로부터 함수, 논리적 이론, 확률 모형을 배우는 방법을 다루었다. 이번 장에서는 에이전트가 자신이 무엇을 해야 하는지를, 그에 대한 이름표 붙은 견본들이 주어지지 않는 상황에서 배우는 방법을 살펴본다.

무엇이 좋고 무엇이 나쁜지에 대한 피드백이 전혀 없으면, 에이전트는 다음 수를 결정하기 위한 근거를 형성할 수 없다.

보상(reward) 또는 강화(reinforcement) : 일종의 피드백


# 수동 강화 학습



# 능동 강화 학습



# 강화 학습의 일반화



# 방침 검색



# 강화 학습의 응용








# 요약

에이전트가 미지의 환경에서 능숙하게 행동하는 방법을 오직 자신의 지각드로가 가끔 주어지는 보상들만으로 배우는 강화 학습 문제를 조사했다. 강화 학습은 인공지능의 모든 문제가 모인 소우주라 할 수 있으나, 이번 장에서는 원활한 논의를 위해 몇 가지 단순화된 설정들에서의 강화 학습을 살펴보았다. 이번 장의 요점은 다음과 같다.

에이전트가 반드시 배워야 하는 정보의 종류는 전반적인 에이전트 설계가 결정한다. 이번 장에서 살펴본 주된 에이전트 설계 세 가지 모형 P와 효용 함수 U를 사용하는 모형 기반 설계와 동작-효용 함수 Q를 사용하고 모형은 필요하지 않은 설계, 방침 𝛑를 사용하는 반사 에이전트 설계이다.

효용을 배우는 접근방식은 다음 3가지이다.

1. 직접 효용 추정은 주어진 상태에 대해 관찰된 총 향후 보상을 효용 학습의 직접적인 증거로 사용한다.
1. 적응성 동적 계획법(ADP)은 관찰들로부터 모형 하나와 보상 함수 하나를 배우고, 평가치 반복이나 방칙 반복을 이용해서 효용들이나 최적 방침을 얻는다. ADP는 환경의 이웃 구조를 통해 가해지는, 상태 효용들에 대한 국소적 제약들을 최적으로 활용한다.
1. 시간 차분(TD) 방법은 효용 추정치들을 후행 상태들에 맞게 갱신한다. TD는 전이 모형 없이도 학습이 가능한, ADP 접근방식의 간단한 근사라 할 수 있다. 학습된 모형을 이용해서 유사 경험들을 생성함으로써 학습 속도를 높일 수 있다.

동작 효용 함수, 즉 Q-함수는 ADP 접근방식으로 배울 수도 있고 TD 접근방식으로 배울 수도 있다. TD를 사용한 Q-학습에서는 학습에도, 동작 선택에도 모형이 필요하지 않다. 이 덕분에 학습 문제가 간단해지지만, 가능한 동작열의 결과들을 에이전트가 시뮬레이션할 수 없기 때문에 복잡한 환경을 배우는 능력이 제한될 수 있다.

학습하는 에이전트가 학습 도중에 동작을 스스로 선택해야 하는 경우에는 그런 동작들의 기대 가치와 유용한 새 정보를 배우는 것의 잠재적 이득 사이의 절충점을 찾아야 한다. 이러한 탐험 문제의 정확한 해답을 계산하는 것은 비현실적이나, 비교적 좋은 결과는 내는 간단한 발견법들이 존재한다.

상태 공간이 클 때, 상태들에 대해 일반화가 일어나려면 강화 학습 알고리즘은 반드시 근사 함수 표현을 사용해야 한다. 신경망 같은 표현의 매개변수들을 갱신할 때에는 시간 차분 신호를 직접 사용할 수 있다.

방침 검색 방법들은 관찰된 성과에 기초해서 방침의 표현을 직접 개선한다. 확률론적 정의역에서는 성과의 변이(variation)가 심각한 문제인데, 시뮬레이션이 가능한 문제 영역에서는 무작위성을 미리 고정시킴으로써 그러한 문제를 극복할 수 있다.

제어 전략을 사람이 직접 손으로 코딩하는 수고를 덜 수 있다는 장점 덕분에, 강화 학습은 기계 학습 분야에서 여전히 가장 활발하게 연구되는 분야 중 하나이다. 로봇공학에서의 응용들은 특히나 가치가 높을 것이다. 로봇공학의 문제들을 풀려면 성공적인 행동들이 수천에서 수백만 개의 기본 동작들로 이루어지는 연속적인 고차원 부분 관찰 가능 환경을 다룰 수 있는 방법이 필요하다.











